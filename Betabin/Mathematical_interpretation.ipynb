{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6f7257",
   "metadata": {},
   "source": [
    "## Beta-Binomial Regression Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc92aa",
   "metadata": {},
   "source": [
    "Beta-binomial regression model accounting for overdispersion in binomial data is one of the simplest Bayesian models. In this package, we perform beta-binomial regression model by means of beta-binomial distribution with a logistic link.\n",
    "\n",
    "Suppose we toss a coin for $N$ trials and observe the number of heads as $y$. The probability of heads is inferred based on the observed data $D$. Let $\\theta \\in [0,1]$ represent the rate parameter (probability of getting a head).\n",
    "\n",
    "We have several ways to estimate the paramters $\\theta$ from observed data $D$. However, these approaches do not account the uncertainty of the estimates and this may cause the problem of overfitting. \n",
    "\n",
    "Hence, if you have proportion data and no need to consider the overdispersion in clustered binomial data, binomial regression model can be adopted. However, if the data is overdispersed and you want to account for the uncertainty of parameter estimation, beta-binomial regression model can be considered. One of the examples is to select the informative clonal SNPs in single cell studies and it is also demonstrated to show how the Betabin package works. You may refer to [documentation.ipynb](https://github.com/StatBiomed/BetabinGLM/blob/main/docs/documentation.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c2546",
   "metadata": {},
   "source": [
    "### 1. Bayesian Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cbba7",
   "metadata": {},
   "source": [
    "Normally, modeling the uncertainty about the parameters can be adopted by using a probability distribution and in Bayesian statistics, the uncertainty is represented by posterior distribuion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7f51a",
   "metadata": {},
   "source": [
    "If you want to estimate the parameter $\\theta$ conditioned by observed data $D$, based on Bayes rule, we have:\n",
    "\n",
    "$$p(\\theta|D) = \\frac{p(\\theta)p(D|\\theta)}{p(D)}\\ = \\frac{p(\\theta)p(D|\\theta)}{\\int\\,p(\\theta')p(D|\\theta')d\\theta'}$$\n",
    "\n",
    "where, \n",
    "\n",
    "$p(\\theta|D)$ is the posterior distribution\n",
    "\n",
    "$p(\\theta)$ is the prior distribution \n",
    "\n",
    "$p(D|\\theta)$ is the likelihood function \n",
    "\n",
    "Therefore, the posterior $p(\\theta|D)$ is computed by conditioning the prior on the observed data $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c7c0bf",
   "metadata": {},
   "source": [
    "### 1.1 Binomial model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b34da4",
   "metadata": {},
   "source": [
    "#### 1.1.1 Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2df124",
   "metadata": {},
   "source": [
    "In binomial distribution, the prior is called beta distribution. Its domain is bounded between 0 and 1. Defined as follows:\n",
    "\n",
    "$$\\mathrm{Beta}(\\theta|\\alpha,\\beta) = \\frac{1}{B(\\alpha,\\beta)}\\ \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b01af3",
   "metadata": {},
   "source": [
    "where $B(\\alpha,\\beta)$ is the beta function, defined by\n",
    "\n",
    "$$\\mathit{B}(\\alpha,\\beta) \\triangleq \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}$$\n",
    "\n",
    "$$B(\\alpha,\\beta) = \\int_0^1 \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}d\\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d080644",
   "metadata": {},
   "source": [
    "where $\\Gamma(\\alpha)$ is the Gamma function defined by\n",
    "\n",
    "$$\\Gamma(\\alpha) \\triangleq \\int_0^\\infty\\!\\theta^{\\alpha-1}e^{-\\theta}d\\theta$$\n",
    "\n",
    "For any positive interger $n$,\n",
    "\n",
    "$$\\Gamma(n) = (n-1)!$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17486160",
   "metadata": {},
   "source": [
    "Hence, the prior for binomial distribution takes the following form:\n",
    "\n",
    "$$p(\\theta) \\propto \\theta^{\\overset\\smile{\\alpha}-1}(1-\\theta)^{\\overset\\smile{\\beta}-1} = \\mathrm{Beta}(\\theta|\\overset\\smile{\\alpha}, \\overset\\smile{\\beta})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5295e",
   "metadata": {},
   "source": [
    "#### 1.1.2 Binomial likelihood\n",
    "\n",
    "Consider the case mentioned at the start of this article. The likelihood for binomial model takes the following form:\n",
    "\n",
    "$$p(\\mathcal{D}|\\theta) = \\mathrm{Bin}(y|N,\\theta) = \\binom {N}{y}\\theta^y(1-\\theta)^{N-y}$$\n",
    "\n",
    "We can see that the likelihood function takes the same form as the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a934d",
   "metadata": {},
   "source": [
    "#### 1.1.3 Posterior \n",
    "Then, based on Bayes rule, we multiply the Bernoulli likelihood (1.1.2) with the beta prior (1.1.1):\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{split}\n",
    "p(\\theta|D)& \\propto \\theta^y(1-\\theta)^{N-y}\\theta^{\\overset{\\smile}{\\alpha}-1}(1-\\theta)^{\\overset{\\smile}{\\beta}-1}\\\\\n",
    "&=\\theta^{y+\\overset{\\smile}{\\alpha}-1}(1-\\theta)^{N-y+\\overset{\\smile}{\\beta}-1}\\\\\n",
    "&\\propto \\mathrm{Beta}(\\theta|y+\\overset{\\smile}{\\alpha}, N-y+\\overset{\\smile}{\\beta})\\\\\n",
    "&= \\mathrm{Beta}(\\theta|\\overset{\\frown}{\\alpha}, \\overset{\\frown}{\\beta})\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "where $\\overset{\\frown}{\\alpha} \\triangleq y+\\overset{\\smile}{\\alpha}$, $\\overset{\\frown}{\\beta} \\triangleq N-y+\\overset{\\smile}{\\beta}$. The parameters $\\alpha$ and $\\beta$ are called hyper-parameters.\n",
    "\n",
    "We can see that the posterior distribution, which is proportional to the product of the prior and the likelihood function, will have the same function as the prior. \n",
    "\n",
    "This property is called conjugacy. And the beta distribution is a conjugate prior for the Bernoulli likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4574e5",
   "metadata": {},
   "source": [
    "#### 1.1.3.1 Posterior mean \n",
    "The posterior mean is a more robust estimate for the uncertain parameter because it integrates over the whole space.\n",
    "\n",
    "If $p(\\theta|D) = \\mathrm{Beta}(\\theta|\\overset\\frown{\\alpha},\\overset\\frown{\\beta})$, then the posterior mean is given by\n",
    "\n",
    "\n",
    "$$\\DeclareMathOperator{\\E}{\\mathbb{E}}\n",
    "\\bar{\\theta} \\triangleq \\mathbb{E}[\\theta|D] = \\frac{\\overset\\frown{\\alpha}}{\\overset\\frown{\\beta} + \\overset\\frown{\\alpha}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce801e2f",
   "metadata": {},
   "source": [
    "#### 1.1.4 Posterior predictive distribution \n",
    "Now, suppose we are interested in predicting the number of heads in $M>1$ future coin tossing trials and we are using the binomial model. The posterior over $\\theta$ is the same as before (1.1.3), and the posterior predictive distribution becomes:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{split}\n",
    "p(y|D,M)& = \\int_0^1 \\mathrm{Bin}(y|M,\\theta)\\mathrm{Beta}(\\theta|\\overset\\frown{\\alpha}, \\overset\\frown{\\beta}) d\\theta \\\\\n",
    "&= \\binom {M}{y}\\frac{1}{B(\\overset\\frown{\\alpha}, \\overset\\frown{\\beta})}\\int_0^1 \\theta^y(1-\\theta)^{M-y}\\theta^{\\overset\\frown{\\alpha}-1}(1-\\theta)^{\\overset\\frown{\\beta}-1}d\\theta \\\\\n",
    "&= \\binom {M}{y}\\frac{1}{B(\\overset\\frown{\\alpha}, \\overset\\frown{\\beta})}\\int_0^1 \\theta^{y+\\overset\\frown{\\alpha}-1}(1-\\theta)^{M-y+\\overset\\frown{\\beta}-1}d\\theta \\\\\n",
    "&= \\binom {M}{y}\\frac{B(y+\\overset\\frown{\\alpha},M-y+\\overset\\frown{\\beta})}{B(\\overset\\frown{\\alpha}, \\overset\\frown{\\beta})}\n",
    "\\end{split}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f771c4",
   "metadata": {},
   "source": [
    "### 1.2 Beta-binomial distribution \n",
    "Finally, we find the posterior predictive distribution shown above. And it is known as the beta-binomial distribution:\n",
    "\n",
    "$$Bb(y|M,\\overset\\frown{\\alpha},\\overset\\frown{\\beta}) \\triangleq \\binom {M}{y} \\frac{B(y+\\overset\\frown{\\alpha},M-y+\\overset\\frown{\\beta})}{B(\\overset\\frown{\\alpha}, \\overset\\frown{\\beta})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7506c",
   "metadata": {},
   "source": [
    "### 2. Beta-binomial regression model\n",
    "\n",
    "For beta-binomial regression model, we have proportion data $\\binom {M}{y}$ as the endogenous variable and $x$ as the exogenous variable (non-linear predictor). Link function is used to fit the data to a linear model. Here, logit function is used as link function, while inverse of logit function called sigmoid function (i.e., $\\sigma(w^\\mathrm{T}x)$) is used to denot the mapping from the linear inputs to the mean of the output. $w$ is the weight vector (bias $b$ is absorbed into $w$ for easier interpretation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d1f26",
   "metadata": {},
   "source": [
    "- Logistic link\n",
    "\n",
    "$$\\sigma(a) = \\frac{1}{1 + e^{-a}}, \\mathrm{where}\\, a = {w^\\mathrm{T}}x$$\n",
    "\n",
    "And $p(y=1|\\theta) = \\sigma(w^\\mathrm{T}x)$ is called logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e50a5",
   "metadata": {},
   "source": [
    "We can define $p(y=1|\\sigma(w^Tx))$ as the posteror mean $\\mathbb{E}[\\theta|D]$ (1.1.3.1) and $\\phi = \\frac{1}{\\alpha+\\beta+1}$, where $\\phi$ is the overdispersion parameter with bounds between 0 and 1.\n",
    "\n",
    "Then, \n",
    "\n",
    "$$p = \\mathbb{E}[\\theta|D] = \\frac{\\alpha}{\\alpha+\\beta}$$\n",
    "\n",
    "After re-arranging the above formula, we can get\n",
    "\n",
    "$$\\alpha = (1-\\phi)p$$ and $$\\qquad\\beta = \\frac{1}{\\phi}(1-p)+p-1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e7b6d",
   "metadata": {},
   "source": [
    "- Beta-binomial distribution\n",
    "\n",
    "Subsitute back into beta-binomial distribution (1.2), we obtain the final version of the beta-binomial distribution with respect to $w$ and $\\phi$ as the parameters.\n",
    "\n",
    "$$p(y|w,\\phi) = \\binom {M}{y} \\frac{B(y+(1-\\phi)\\sigma(w^\\mathrm{T}x),M-y+\\frac{1}{\\phi}(1-\\sigma(w^\\mathrm{T}x))+\\sigma(w^\\mathrm{T}x)-1)}{B((1-\\phi)\\sigma(w^\\mathrm{T}x), \\frac{1}{\\phi}(1-\\sigma(w^\\mathrm{T}x))+\\sigma(w^\\mathrm{T}x)-1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ba5c3",
   "metadata": {},
   "source": [
    "- Joint probability for beta-binomial distribution (likelihood)\n",
    "\n",
    "$$p(y^{(i)}|\\Theta) = \\prod \\limits _{i=1}^{n} \\binom {M^{(i)}} {y^{(i)}} \\frac{B(y^{(i)}+\\alpha,M^{(i)}-y^{(i)}+\\beta)}{B(\\alpha, \\beta)}$$\n",
    "\n",
    "- Log-likelihood (LL)\n",
    "\n",
    "$$L(\\Theta) = \\sum \\limits _{i=1}^{n} \\mathrm{log} \\binom {M^{(i)}} {y^{(i)}} + \\mathrm{Betaln}(M^{(i)}-y^{(i)}+\\beta, y^{(i)}+\\alpha) - \\mathrm{Betaln}(\\alpha, \\beta)$$\n",
    "\n",
    "- Objective function / cost function\n",
    "\n",
    "$$\\mathrm{Cost} = -[\\mathrm{Betaln}(M^{(i)}-y^{(i)}+\\beta, y^{(i)}+\\alpha) - \\mathrm{Betaln}(\\alpha, \\beta)]$$\n",
    "\n",
    "\n",
    "In the above function, $\\Theta$ represents the overall parameters (i.e.,$w, \\phi$) that we are interested in. \n",
    "\n",
    "Hence, we will find the parameters which maximize the likelihood function with the optimizer (i.e., scipy.optimize.minimize). In other words, as we maximize the LL or LLH, we are also minimizing the cost. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
